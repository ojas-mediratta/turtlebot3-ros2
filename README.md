# ROS 2 and TurtleBot3 ‚Äî Coursework, Projects & Experiments

This repository contains my personal coursework, experiments, and research explorations with the TurtleBot3 platform under **ROS 2**, primarily for **CS 7785: Introduction to Robotics Research** at Georgia Tech.  
It is intended as a personal record and reference, **not** a solutions repository.

---

## üõ†Ô∏è Tech & Tools

- **Languages**: Python, C++
- **Frameworks / Libraries**: ROS 2 Humble (or compatible), OpenCV, PyTorch / TensorFlow  
- **Platforms / Simulators**: TurtleBot3 (real or simulated), Gazebo, RQT / RViz

---

## üöÄ Goals & Themes

- Build hands-on skills in robotics research workflows  
- Explore algorithms spanning perception, planning, and control  
- Maintain reproducible workflows and documented experiments  

---

## üìÇ Repository Layout & Lab Overviews

- `lab2_ws/` ‚Äî ROS2 workspace and code for Lab 2 experiments  
- `lab3_ws/` ‚Äî ROS2 workspace and code for Lab 3  
- `lab4_ws/` ‚Äî ROS2 workspace and code for Lab 4  
- `README.md` ‚Äî this file  

---

<details>
<summary><b>Lab 2 ‚Äì Perception & Object Tracking (`lab2_ws`)</b></summary>

**Purpose**: Set up a ROS2 perception pipeline to detect and track a colored object, publish detection outputs, and link to simple robot motion.

**Key files and modules:**
- `find_object.py` ‚Äî Subscribes to an image topic, applies HSV thresholding and contour detection, and publishes both processed images and object coordinates.  
- `rotate_robot.py` ‚Äî Publishes `Twist` commands to `/cmd_vel` to rotate the robot for verification.  
- Launch/config files ‚Äî Bring up camera, perception, and control nodes with adjustable thresholds.

</details>

---

<details>
<summary><b>Lab 3 ‚Äì Sensor Fusion, PID Control & Object Chasing (`lab3_ws`)</b></summary>

**Purpose**: Combine camera and LIDAR sensing for a closed-loop control system that chases a detected object.  

**Key files and modules:**
- **Vision / Detection Node** ‚Äî Extracts the angular bearing of a tracked object.  
- **Range / Scan Node** ‚Äî Processes `/scan` data to determine object distance.  
- **Chasing Controller** ‚Äî Implements cascaded PIDs for angular and distance control, publishing `Twist` messages to `/cmd_vel`.  
- Launch/config files ‚Äî Integrate all nodes with tuned parameters for stability and response.

</details>

---

<details>
<summary><b>Lab 4 ‚Äì Go-to-Goal with Obstacle Avoidance (`lab4_ws`)</b></summary>

**Purpose**: Develop autonomous navigation behaviors using odometry and LIDAR data to reach a goal position while avoiding obstacles.

**Key files and modules:**
- `get_object_range.py` ‚Äî Subscribes to `/scan`, detects nearby obstacles, and publishes a vector to the closest object as `/obstacle_vector` (`geometry_msgs/Vector3`).  
- `go_to_goal.py` ‚Äî Subscribes to `/odom` and `/obstacle_vector`, computes velocity commands for goal-directed navigation with reactive obstacle avoidance, and publishes `/cmd_vel` (`geometry_msgs/Twist`).  
- `turtlebot3_bringup.launch.py` ‚Äî Initializes onboard drivers and sensors, including `/scan` and `/odom` publishers.  
- Launch configuration ‚Äî Integrates all nodes into a single pipeline for autonomous navigation.  
- See the `lab4_ws/README.md` for the full computational diagram.

</details>

---

## ‚ö†Ô∏è Academic Integrity & Usage

Everything in this repository is **my original work** for CS 7785.  
This is **not** a solutions manual and should **not** be used for direct submission in any course.  
If you are a student, please follow the **Georgia Tech Honor Code** ‚Äî use this repository *only* for learning, reference **after** you‚Äôve done your own work, or inspiration (not copy-paste).
